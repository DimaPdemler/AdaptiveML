{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "import copy\n",
    "# from summary import summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network parameters: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc1 = nn.Linear(400,120)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "model = LeNet().to(device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Mnist dataset, training method, and testing method ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant variables for the ML task\n",
    "batch_size = 100\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "#Loading the dataset and preprocessing\n",
    "train_dataset = torchvision.datasets.MNIST(root = '/Users/dimademler/Desktop/me-Programming/pytorch',\n",
    "                                           train = True,\n",
    "                                           transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
    "                                           download = True)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root = '/Users/dimademler/Desktop/me-Programming/pytorch',\n",
    "                                          train = False,\n",
    "                                          transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
    "                                          download=True)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "#Setting the loss function\n",
    "cost = nn.CrossEntropyLoss()\n",
    "\n",
    "#this is defined to print how many steps are remaining when training\n",
    "total_step = len(train_loader)\n",
    "\n",
    "def train(model,train_loader,test_loader,num_epochs,optimizer):\n",
    "  total_step = len(train_loader)\n",
    "  for epoch in range(num_epochs):\n",
    "      for i, (images, labels) in enumerate(train_loader):  \n",
    "          images = images.to(device)\n",
    "          labels = labels.to(device)\n",
    "          \n",
    "          #Forward pass\n",
    "          outputs = model(images)\n",
    "          loss = cost(outputs, labels)\n",
    "            \n",
    "          # Backward and optimize\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "              \n",
    "          if (i+1) % 400 == 0:\n",
    "              print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                        .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "  \n",
    "def test(model, test_loader):\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(test_loader):  \n",
    "            test_output = model(images)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: %.2f' % accuracy)\n",
    "    return accuracy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving initial model weights ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = model.state_dict()\n",
    "torch.save(initial_state, \"initial_model_state.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Network ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Step [400/600], Loss: 0.0829\n",
      "Epoch [2/6], Step [400/600], Loss: 0.0844\n",
      "Epoch [3/6], Step [400/600], Loss: 0.0146\n",
      "Epoch [4/6], Step [400/600], Loss: 0.0182\n",
      "Epoch [5/6], Step [400/600], Loss: 0.0054\n",
      "Epoch [6/6], Step [400/600], Loss: 0.0045\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 6\n",
    "\n",
    "model = LeNet()\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "train(model, train_loader,test_loader,num_epochs, optimizer = optimizer)\n",
    "# modeloriginal= copy.deepcopy(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 2 at making iterative pruning ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timespruned=30\n",
    "num_epochs=10\n",
    "k= 15 # number of epochs trained before resetting\n",
    "\n",
    "\n",
    "# First training\n",
    "train(model, train_loader,test_loader,num_epochs=k, optimizer = optimizer)\n",
    "\n",
    "parameters_to_prune2 = (\n",
    "    (model.fc1, 'weight'),\n",
    "    (model.fc2, 'weight'),\n",
    ")\n",
    "\n",
    "prev_state = model.state_dict()\n",
    "torch.save(prev_state, \"initial_model_state.pth\")\n",
    "\n",
    "prune.global_unstructured(parameters_to_prune2,prune.L1Unstructured, importance_scores=None, amount=0.2)\n",
    "\n",
    "for i in range(num_timespruned):\n",
    "    \n",
    "    \n",
    "    # model.load_state_dict(initial_state)\n",
    "\n",
    "    # Saving masks method from Luke\n",
    "    masks = []\n",
    "    for module, _ in parameters_to_prune2:\n",
    "      masks.append(module.get_buffer('weight_mask').data)\n",
    "    model.load_state_dict(torch.load(prev_state))\n",
    "    for i, (module, _) in enumerate(parameters_to_prune2):\n",
    "      prune.custom_from_mask(module,'weight',masks[i])\n",
    "    \n",
    "    prune.global_unstructured(parameters_to_prune2,prune.L1Unstructured, importance_scores=None, amount=0.2)\n",
    "\n",
    "    train(model, train_loader,test_loader,num_epochs = num_epochs, optimizer = optimizer)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ticket(model, parameters_to_prune, name, train_loader, test_loader, start_iter = 0, end_iter = 30, num_epochs = 4, learning_rate = .001, prune_amount = .2, k = 1):\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  train(model, train_loader,test_loader,num_epochs = k, optimizer = optimizer) #Kth epoch save weights\n",
    "  torch.save(model.state_dict(), 'K_iterweights.pth')\n",
    "  train(model, train_loader,test_loader,num_epochs = num_epochs - k, optimizer = optimizer) #Pretrain\n",
    "\n",
    "  accuracy = []\n",
    "\n",
    "  for i in range(start_iter, end_iter):  \n",
    "    #Prune\n",
    "    prune.global_unstructured(parameters_to_prune,pruning_method=prune.L1Unstructured,amount=prune_amount,)\n",
    "\n",
    "    #Save Masks, Rewind Weights, then place Masks back\n",
    "    masks = []\n",
    "    for module, _ in parameters_to_prune:\n",
    "      masks.append(module.get_buffer('weight_mask').data)\n",
    "    model.load_state_dict(torch.load('K_iterweights.pth'))\n",
    "    for i, (module, _) in enumerate(parameters_to_prune):\n",
    "      prune.custom_from_mask(module,'weight',masks[i])\n",
    "\n",
    "    #Train\n",
    "    train(model, train_loader,test_loader,num_epochs = num_epochs, optimizer = optimizer)\n",
    "    accuracy.append(test(model, test_loader))\n",
    "\n",
    "    plt.plot(np.arange(len(accuracy)), accuracy)\n",
    "    plt.show()\n",
    "    print('Saving iteration ', str(i+1))\n",
    "    torch.save(model.state_dict(), name + '_iter' + str(i+1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [400/600], Loss: 0.0625\n",
      "Epoch [1/9], Step [400/600], Loss: 0.0608\n",
      "Epoch [2/9], Step [400/600], Loss: 0.0336\n",
      "Epoch [3/9], Step [400/600], Loss: 0.0066\n",
      "Epoch [4/9], Step [400/600], Loss: 0.0478\n",
      "Epoch [5/9], Step [400/600], Loss: 0.0011\n",
      "Epoch [6/9], Step [400/600], Loss: 0.0072\n",
      "Epoch [7/9], Step [400/600], Loss: 0.0017\n",
      "Epoch [8/9], Step [400/600], Loss: 0.0202\n",
      "Epoch [9/9], Step [400/600], Loss: 0.0055\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LeNet:\n\tMissing key(s) in state_dict: \"fc1.weight_orig\", \"fc1.weight_mask\", \"fc2.weight_orig\", \"fc2.weight_mask\", \"fc3.weight_orig\", \"fc3.weight_mask\". \n\tUnexpected key(s) in state_dict: \"fc1.weight\", \"fc2.weight\", \"fc3.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m LeNet()\n\u001b[1;32m      2\u001b[0m parameters_to_prune3 \u001b[39m=\u001b[39m (\n\u001b[1;32m      3\u001b[0m     (model\u001b[39m.\u001b[39mfc1, \u001b[39m'\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m      4\u001b[0m     (model\u001b[39m.\u001b[39mfc2, \u001b[39m'\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m      5\u001b[0m     (model\u001b[39m.\u001b[39mfc3, \u001b[39m'\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m find_ticket(model, parameters_to_prune3, \u001b[39m'\u001b[39;49m\u001b[39mMNIST\u001b[39;49m\u001b[39m'\u001b[39;49m, train_loader, test_loader)\n",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m, in \u001b[0;36mfind_ticket\u001b[0;34m(model, parameters_to_prune, name, train_loader, test_loader, start_iter, end_iter, num_epochs, learning_rate, prune_amount, k)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m module, _ \u001b[39min\u001b[39;00m parameters_to_prune:\n\u001b[1;32m     16\u001b[0m   masks\u001b[39m.\u001b[39mappend(module\u001b[39m.\u001b[39mget_buffer(\u001b[39m'\u001b[39m\u001b[39mweight_mask\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mdata)\n\u001b[0;32m---> 17\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mK_iterweights.pth\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m i, (module, _) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(parameters_to_prune):\n\u001b[1;32m     19\u001b[0m   prune\u001b[39m.\u001b[39mcustom_from_mask(module,\u001b[39m'\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m'\u001b[39m,masks[i])\n",
      "File \u001b[0;32m~/Desktop/me-Programming/envirs/tensorflow-test/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1667\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1668\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1670\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1671\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1672\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1673\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LeNet:\n\tMissing key(s) in state_dict: \"fc1.weight_orig\", \"fc1.weight_mask\", \"fc2.weight_orig\", \"fc2.weight_mask\", \"fc3.weight_orig\", \"fc3.weight_mask\". \n\tUnexpected key(s) in state_dict: \"fc1.weight\", \"fc2.weight\", \"fc3.weight\". "
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "parameters_to_prune3 = (\n",
    "    (model.fc1, 'weight'),\n",
    "    (model.fc2, 'weight'),\n",
    "    (model.fc3, 'weight'),\n",
    ")\n",
    "find_ticket(model, parameters_to_prune3, 'MNIST', train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of 10 friends and print them"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempting iterative pruning ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing after global pruning: \n",
      "Test Accuracy of the model on the 10000 test images: 0.20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modelcopy2= copy.deepcopy(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_timespruned=30\n",
    "\n",
    "parameters_to_prune2 = (\n",
    "    (model.fc1, 'weight'),\n",
    "    (model.fc2, 'weight'),\n",
    ")\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune2,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.1\n",
    ")\n",
    "# run pruning once\n",
    "# prune.global_unstructured.pruning_method\n",
    "  # torch.nn.utils.prune.random_unstructured(module, name, num_prunedfloat)\n",
    "\n",
    "# torch.nn.utils.prune.global_unstructured(parameters_to_prune2, )\n",
    "\n",
    "print(\"Testing after global pruning: \")\n",
    "test(model,test_loader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(num_timespruned):\n",
    "    \n",
    "    prune.global_unstructured(parameters_to_prune2,prune.L1Unstructured, importance_scores=None, amount=0.2)\n",
    "    model.load_state_dict(initial_state)\n",
    "    \n",
    "    # Need to save masks somehow and load them in\n",
    "    train(model, train_loader,test_loader,num_epochs, optimizer = optimizer)\n",
    "    print(\"pruning number: \", i)\n",
    "    test(model,test_loader)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # maskweights_fc1=model.fc1.named_buffers()\n",
    "\n",
    "#   modelretrain=  LeNet().to(device=device)\n",
    "#   modelretrain.load_state_dict(torch.load(\"modelcopy2_state.pth\"))\n",
    "\n",
    "\n",
    "# fc1mask=modelcopy2.fc1\n",
    "# fc2mask=modelcopy2.fc2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing after pruning: \n",
      "Test Accuracy of the model on the 10000 test images: 1.00\n",
      "[('weight_mask', tensor([[1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 0.]]))]\n"
     ]
    }
   ],
   "source": [
    "modelcopy= copy.deepcopy(model)\n",
    "\n",
    "# max_growth = 30\n",
    "num_prunedfloat=0.2\n",
    "\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (modelcopy.fc1, 'weight'),\n",
    "    (modelcopy.fc2, 'weight'),\n",
    ")\n",
    "\n",
    "for module,name in parameters_to_prune:\n",
    "  torch.nn.utils.prune.random_unstructured(module, name, num_prunedfloat)\n",
    "\n",
    "\n",
    "print(\"Testing after pruning: \")\n",
    "test(modelcopy,test_loader)\n",
    "\n",
    "module2 = modelcopy.fc2\n",
    "# print(list(module2.named_buffers()))\n",
    "# print(list(module2.weight))\n",
    "# vgg = models.vgg16()\n",
    "# summary(vgg, (3, 224, 224))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [400/600], Loss: 0.0537\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0773\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0305\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0182\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 34\u001b[0m\n\u001b[1;32m     22\u001b[0m baseline_acc \u001b[39m=\u001b[39m []\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[1;32m     24\u001b[0m   \u001b[39m#grow 1/i*growth_factor params total:\u001b[39;00m\n\u001b[1;32m     25\u001b[0m   \n\u001b[1;32m     26\u001b[0m   \u001b[39m#for module,name in parameters_to_prune:\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[39m#grow\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m   train(model, train_loader,test_loader,num_epochs \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m, optimizer \u001b[39m=\u001b[39;49m optimizer)\n\u001b[1;32m     35\u001b[0m   \u001b[39m# train(baseline, train_loader,test_loader,num_epochs = 5, optimizer = baseline_optim)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m   \u001b[39m# model_acc.append(test(model, test_loader))\u001b[39;00m\n\u001b[1;32m     37\u001b[0m   baseline_acc\u001b[39m.\u001b[39mappend(test(baseline, test_loader))\n",
      "Cell \u001b[0;32mIn[3], line 53\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, num_epochs, optimizer)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m# Backward and optimize\u001b[39;00m\n\u001b[1;32m     52\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 53\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     54\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     56\u001b[0m \u001b[39mif\u001b[39;00m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m400\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/me-Programming/envirs/tensorflow-test/env/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/me-Programming/envirs/tensorflow-test/env/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_growth = 20\n",
    "num_epochs = 10\n",
    "gamma = 10 #num new nodes on first growth\n",
    "\n",
    "model = LeNet(max_growth = max_growth)\n",
    "baseline = LeNet(max_growth = 0)\n",
    "\n",
    "parameters_to_prune = (\n",
    "#    (model.conv1, 'weight'),\n",
    "#    (model.conv2, 'weight'),\n",
    "    (model.fc1, 'weight'),\n",
    "    (model.fc2, 'weight'),\n",
    "#    (model.fc3, 'weight'),\n",
    ")\n",
    "for module,name in parameters_to_prune:\n",
    "  torch.nn.utils.prune.random_structured(module, name, max_growth,  0)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "baseline_optim = torch.optim.Adam(baseline.parameters(), lr=learning_rate)\n",
    "\n",
    "model_acc = []\n",
    "baseline_acc = []\n",
    "for i in range(5):\n",
    "  #grow 1/i*growth_factor params total:\n",
    "  \n",
    "  #for module,name in parameters_to_prune:\n",
    "    #grow\n",
    "  \n",
    "    \n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "  train(model, train_loader,test_loader,num_epochs = 5, optimizer = optimizer)\n",
    "  # train(baseline, train_loader,test_loader,num_epochs = 5, optimizer = baseline_optim)\n",
    "  # model_acc.append(test(model, test_loader))\n",
    "  baseline_acc.append(test(baseline, test_loader))\n",
    "\n",
    "  plt.plot(np.arange(len(model_acc)), model_acc, label = 'Adaptive Model')\n",
    "  plt.plot(np.arange(len(baseline_acc)), baseline_acc, label = 'Baseline')\n",
    "  plt.show()\n",
    "\n",
    "  prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.10+(-1*i*.01),\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "065903266eccafe089f2ae6c8a75912f88be57d9e6a3d93d1ff3f958981ce42e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
