{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "import copy\n",
    "# from summary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, max_growth):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc1 = nn.Linear(400,120+max_growth)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(120+max_growth,84+max_growth)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(84+max_growth,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "model = LeNet(20).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant variables for the ML task\n",
    "batch_size = 100\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "#Loading the dataset and preprocessing\n",
    "train_dataset = torchvision.datasets.MNIST(root = '/Users/dimademler/Desktop/me-Programming/pytorch',\n",
    "                                           train = True,\n",
    "                                           transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
    "                                           download = True)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root = '/Users/dimademler/Desktop/me-Programming/pytorch',\n",
    "                                          train = False,\n",
    "                                          transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
    "                                          download=True)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "#Setting the loss function\n",
    "cost = nn.CrossEntropyLoss()\n",
    "\n",
    "#this is defined to print how many steps are remaining when training\n",
    "total_step = len(train_loader)\n",
    "\n",
    "def train(model,train_loader,test_loader,num_epochs,optimizer):\n",
    "  total_step = len(train_loader)\n",
    "  for epoch in range(num_epochs):\n",
    "      for i, (images, labels) in enumerate(train_loader):  \n",
    "          images = images.to(device)\n",
    "          labels = labels.to(device)\n",
    "          \n",
    "          #Forward pass\n",
    "          outputs = model(images)\n",
    "          loss = cost(outputs, labels)\n",
    "            \n",
    "          # Backward and optimize\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "              \n",
    "          if (i+1) % 400 == 0:\n",
    "              print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                        .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "  \n",
    "def test(model, test_loader):\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(test_loader):  \n",
    "            test_output = model(images)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: %.2f' % accuracy)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_growth = 20\n",
    "num_epochs = 10\n",
    "\n",
    "# train(model, train_loader,test_loader,num_epochs = 5, optimizer = optimizer)\n",
    "# test(model,test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Step [400/600], Loss: 0.0163\n",
      "Epoch [2/6], Step [400/600], Loss: 0.0580\n",
      "Epoch [3/6], Step [400/600], Loss: 0.0425\n",
      "Epoch [4/6], Step [400/600], Loss: 0.0245\n",
      "Epoch [5/6], Step [400/600], Loss: 0.0380\n",
      "Epoch [6/6], Step [400/600], Loss: 0.0034\n"
     ]
    }
   ],
   "source": [
    "max_growth = 30\n",
    "num_epochs = 6\n",
    "\n",
    "model = LeNet(max_growth = max_growth)\n",
    "# baseline = LeNet(max_growth = 0)\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (model.fc1, 'weight'),\n",
    "    (model.fc2, 'weight'),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# baseline_optim = torch.optim.Adam(baseline.parameters(), lr=learning_rate)\n",
    "\n",
    "model_acc = []\n",
    "baseline_acc = []\n",
    "\n",
    "train(model, train_loader,test_loader,num_epochs, optimizer = optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing before pruning: \n",
      "Test Accuracy of the model on the 10000 test images: 1.00\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing before pruning: \")\n",
    "\n",
    "# print(LeNet)\n",
    "test(model,test_loader)\n",
    "vgg = models.vgg16()\n",
    "# summary(vgg, (3, 224, 224))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing after pruning: \n",
      "Test Accuracy of the model on the 10000 test images: 0.62\n",
      "layer1.0.weight \t torch.Size([6, 1, 5, 5])\n",
      "layer1.0.bias \t torch.Size([6])\n",
      "layer1.1.weight \t torch.Size([6])\n",
      "layer1.1.bias \t torch.Size([6])\n",
      "layer1.1.running_mean \t torch.Size([6])\n",
      "layer1.1.running_var \t torch.Size([6])\n",
      "layer1.1.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.weight \t torch.Size([16, 6, 5, 5])\n",
      "layer2.0.bias \t torch.Size([16])\n",
      "layer2.1.weight \t torch.Size([16])\n",
      "layer2.1.bias \t torch.Size([16])\n",
      "layer2.1.running_mean \t torch.Size([16])\n",
      "layer2.1.running_var \t torch.Size([16])\n",
      "layer2.1.num_batches_tracked \t torch.Size([])\n",
      "fc1.bias \t torch.Size([150])\n",
      "fc1.weight_orig \t torch.Size([150, 400])\n",
      "fc1.weight_mask \t torch.Size([150, 400])\n",
      "fc2.bias \t torch.Size([114])\n",
      "fc2.weight_orig \t torch.Size([114, 150])\n",
      "fc2.weight_mask \t torch.Size([114, 150])\n",
      "fc3.weight \t torch.Size([10, 114])\n",
      "fc3.bias \t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "modelcopy= copy.deepcopy(model)\n",
    "\n",
    "# max_growth = 30\n",
    "num_prunedfloat=0.7\n",
    "\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (modelcopy.fc1, 'weight'),\n",
    "    (modelcopy.fc2, 'weight'),\n",
    ")\n",
    "\n",
    "for module,name in parameters_to_prune:\n",
    "  torch.nn.utils.prune.random_structured(module, name, num_prunedfloat,  0)\n",
    "\n",
    "print(\"Testing after pruning: \")\n",
    "test(modelcopy,test_loader)\n",
    "# vgg = models.vgg16()\n",
    "# summary(vgg, (3, 224, 224))\n",
    "\n",
    "# print(model.state_dict())\n",
    "for param_tensor in modelcopy.state_dict():\n",
    "    print(param_tensor, \"\\t\", modelcopy.state_dict()[param_tensor].size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [400/600], Loss: 0.0537\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0773\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0305\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0182\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 34\u001b[0m\n\u001b[1;32m     22\u001b[0m baseline_acc \u001b[39m=\u001b[39m []\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[1;32m     24\u001b[0m   \u001b[39m#grow 1/i*growth_factor params total:\u001b[39;00m\n\u001b[1;32m     25\u001b[0m   \n\u001b[1;32m     26\u001b[0m   \u001b[39m#for module,name in parameters_to_prune:\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[39m#grow\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m   train(model, train_loader,test_loader,num_epochs \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m, optimizer \u001b[39m=\u001b[39;49m optimizer)\n\u001b[1;32m     35\u001b[0m   \u001b[39m# train(baseline, train_loader,test_loader,num_epochs = 5, optimizer = baseline_optim)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m   \u001b[39m# model_acc.append(test(model, test_loader))\u001b[39;00m\n\u001b[1;32m     37\u001b[0m   baseline_acc\u001b[39m.\u001b[39mappend(test(baseline, test_loader))\n",
      "Cell \u001b[0;32mIn[3], line 53\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, num_epochs, optimizer)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m# Backward and optimize\u001b[39;00m\n\u001b[1;32m     52\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 53\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     54\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     56\u001b[0m \u001b[39mif\u001b[39;00m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m400\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/me-Programming/envirs/tensorflow-test/env/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/me-Programming/envirs/tensorflow-test/env/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_growth = 20\n",
    "num_epochs = 10\n",
    "gamma = 10 #num new nodes on first growth\n",
    "\n",
    "model = LeNet(max_growth = max_growth)\n",
    "baseline = LeNet(max_growth = 0)\n",
    "\n",
    "parameters_to_prune = (\n",
    "#    (model.conv1, 'weight'),\n",
    "#    (model.conv2, 'weight'),\n",
    "    (model.fc1, 'weight'),\n",
    "    (model.fc2, 'weight'),\n",
    "#    (model.fc3, 'weight'),\n",
    ")\n",
    "for module,name in parameters_to_prune:\n",
    "  torch.nn.utils.prune.random_structured(module, name, max_growth,  0)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "baseline_optim = torch.optim.Adam(baseline.parameters(), lr=learning_rate)\n",
    "\n",
    "model_acc = []\n",
    "baseline_acc = []\n",
    "for i in range(5):\n",
    "  #grow 1/i*growth_factor params total:\n",
    "  \n",
    "  #for module,name in parameters_to_prune:\n",
    "    #grow\n",
    "  \n",
    "    \n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "  train(model, train_loader,test_loader,num_epochs = 5, optimizer = optimizer)\n",
    "  # train(baseline, train_loader,test_loader,num_epochs = 5, optimizer = baseline_optim)\n",
    "  # model_acc.append(test(model, test_loader))\n",
    "  baseline_acc.append(test(baseline, test_loader))\n",
    "\n",
    "  plt.plot(np.arange(len(model_acc)), model_acc, label = 'Adaptive Model')\n",
    "  plt.plot(np.arange(len(baseline_acc)), baseline_acc, label = 'Baseline')\n",
    "  plt.show()\n",
    "\n",
    "  prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.10+(-1*i*.01),\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "065903266eccafe089f2ae6c8a75912f88be57d9e6a3d93d1ff3f958981ce42e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
