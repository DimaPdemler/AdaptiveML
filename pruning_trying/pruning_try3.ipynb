{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "import copy\n",
    "# from summary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc1 = nn.Linear(400,120)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "model = LeNet().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant variables for the ML task\n",
    "batch_size = 100\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "#Loading the dataset and preprocessing\n",
    "train_dataset = torchvision.datasets.MNIST(root = '/Users/dimademler/Desktop/me-Programming/pytorch',\n",
    "                                           train = True,\n",
    "                                           transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
    "                                           download = True)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root = '/Users/dimademler/Desktop/me-Programming/pytorch',\n",
    "                                          train = False,\n",
    "                                          transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
    "                                          download=True)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "#Setting the loss function\n",
    "cost = nn.CrossEntropyLoss()\n",
    "\n",
    "#this is defined to print how many steps are remaining when training\n",
    "total_step = len(train_loader)\n",
    "\n",
    "def train(model,train_loader,test_loader,num_epochs,optimizer):\n",
    "  total_step = len(train_loader)\n",
    "  for epoch in range(num_epochs):\n",
    "      for i, (images, labels) in enumerate(train_loader):  \n",
    "          images = images.to(device)\n",
    "          labels = labels.to(device)\n",
    "          \n",
    "          #Forward pass\n",
    "          outputs = model(images)\n",
    "          loss = cost(outputs, labels)\n",
    "            \n",
    "          # Backward and optimize\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "              \n",
    "          if (i+1) % 400 == 0:\n",
    "              print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                        .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "  \n",
    "def test(model, test_loader):\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(test_loader):  \n",
    "            test_output = model(images)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: %.2f' % accuracy)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Step [400/600], Loss: 0.0765\n",
      "Epoch [2/6], Step [400/600], Loss: 0.0085\n",
      "Epoch [3/6], Step [400/600], Loss: 0.0045\n",
      "Epoch [4/6], Step [400/600], Loss: 0.0140\n",
      "Epoch [5/6], Step [400/600], Loss: 0.0137\n",
      "Epoch [6/6], Step [400/600], Loss: 0.0678\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 6\n",
    "\n",
    "model = LeNet()\n",
    "# baseline = LeNet(max_growth = 0)\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (model.fc1, 'weight'),\n",
    "    (model.fc2, 'weight'),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# baseline_optim = torch.optim.Adam(baseline.parameters(), lr=learning_rate)\n",
    "\n",
    "model_acc = []\n",
    "baseline_acc = []\n",
    "\n",
    "train(model, train_loader,test_loader,num_epochs, optimizer = optimizer)\n",
    "modeloriginal= copy.deepcopy(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = model.fc1\n",
    "# print(list(module.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing before pruning: \n",
      "Test Accuracy of the model on the 10000 test images: 0.99\n",
      "OrderedDict([('layer1.0.weight', tensor([[[[ 0.0780,  0.0925,  0.2535, -0.0266,  0.0510],\n",
      "          [-0.1898, -0.1547,  0.0811,  0.1534,  0.1783],\n",
      "          [ 0.1082, -0.1937, -0.0041, -0.0751,  0.1615],\n",
      "          [ 0.0507, -0.2013, -0.1807, -0.1799, -0.0739],\n",
      "          [ 0.1296, -0.0833, -0.0534,  0.0724, -0.2185]]],\n",
      "\n",
      "\n",
      "        [[[-0.0978, -0.1907, -0.0294, -0.2541, -0.2660],\n",
      "          [ 0.0935, -0.0230, -0.1997, -0.1020,  0.0864],\n",
      "          [-0.0774, -0.0897,  0.0165,  0.2444,  0.2287],\n",
      "          [-0.1112,  0.2113,  0.1765, -0.0452, -0.1106],\n",
      "          [ 0.0113, -0.0447,  0.0612,  0.0520,  0.1021]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2650, -0.0685,  0.0216, -0.0378,  0.0835],\n",
      "          [ 0.1460,  0.1539, -0.0546,  0.0831, -0.0410],\n",
      "          [ 0.0427, -0.0256,  0.1388, -0.1371, -0.0353],\n",
      "          [-0.0097, -0.0101, -0.1266, -0.0485, -0.1070],\n",
      "          [ 0.1089, -0.1521, -0.1577, -0.1783, -0.1391]]],\n",
      "\n",
      "\n",
      "        [[[-0.0146, -0.0330,  0.1622,  0.1797, -0.2428],\n",
      "          [ 0.0414,  0.0364, -0.0487, -0.1305, -0.2824],\n",
      "          [ 0.0243,  0.1268, -0.1119, -0.3813,  0.0390],\n",
      "          [-0.0514,  0.0072, -0.2801,  0.0072,  0.0901],\n",
      "          [-0.0068, -0.2513, -0.2034, -0.1027,  0.1876]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0463,  0.1078,  0.0497, -0.1747, -0.1865],\n",
      "          [-0.0318,  0.1978, -0.0408, -0.1980, -0.0154],\n",
      "          [ 0.0973,  0.1375, -0.0676, -0.0600, -0.2358],\n",
      "          [ 0.0206,  0.1769, -0.1573, -0.1451, -0.0561],\n",
      "          [ 0.0653, -0.0563, -0.1441, -0.0405,  0.0487]]],\n",
      "\n",
      "\n",
      "        [[[-0.0198, -0.2280,  0.0515,  0.1500,  0.1657],\n",
      "          [ 0.0177,  0.0700,  0.2321,  0.0724, -0.1382],\n",
      "          [ 0.0505,  0.1263, -0.1084, -0.2125,  0.0645],\n",
      "          [ 0.1041,  0.0517,  0.0347, -0.2726, -0.1168],\n",
      "          [ 0.0740, -0.0712,  0.1311,  0.0542, -0.0751]]]])), ('layer1.0.bias', tensor([-0.0498, -0.0128, -0.1431, -0.1639, -0.0434, -0.0904])), ('layer1.1.weight', tensor([1.0220, 1.0015, 0.9422, 1.2124, 0.9809, 0.9238])), ('layer1.1.bias', tensor([ 0.0030, -0.1257, -0.2246,  0.0368, -0.1292, -0.1196])), ('layer1.1.running_mean', tensor([-0.0837, -0.0532, -0.1822, -0.3169, -0.1203, -0.0667])), ('layer1.1.running_var', tensor([0.4261, 0.6051, 0.7496, 1.5163, 0.9438, 0.2779])), ('layer1.1.num_batches_tracked', tensor(3600)), ('layer2.0.weight', tensor([[[[ 4.7287e-02,  8.8006e-03,  1.0069e-01, -5.8489e-02, -1.0846e-01],\n",
      "          [-1.1333e-01, -1.2151e-01, -1.0021e-01, -8.9689e-03, -5.8603e-03],\n",
      "          [-3.6252e-02,  2.5596e-02, -5.4723e-02, -5.3246e-02, -1.7884e-02],\n",
      "          [ 1.0153e-01,  1.7266e-01,  1.5187e-01,  9.5203e-02, -7.8983e-02],\n",
      "          [-4.1728e-03,  5.6441e-02,  6.7122e-02,  8.6876e-03,  8.8394e-02]],\n",
      "\n",
      "         [[-6.7861e-02, -8.4586e-02, -4.8519e-02,  5.8017e-02,  6.2957e-02],\n",
      "          [-5.9172e-03,  5.4151e-02,  7.5054e-02,  1.2055e-01, -2.5792e-02],\n",
      "          [ 4.5083e-02,  1.0000e-01,  2.8482e-02,  3.7865e-03,  3.7311e-02],\n",
      "          [-4.1796e-03, -8.6728e-03, -3.0802e-02,  1.2414e-01,  1.3550e-01],\n",
      "          [-1.0442e-01, -2.3665e-02,  5.4058e-02, -1.0361e-01, -9.8810e-02]],\n",
      "\n",
      "         [[-3.2080e-02, -2.2547e-02, -2.5535e-02, -7.6939e-02, -5.6749e-02],\n",
      "          [-1.1859e-01,  3.3121e-02, -1.3623e-02,  8.5418e-04, -1.0708e-02],\n",
      "          [ 6.5817e-02,  4.2833e-02,  1.0003e-01,  8.1411e-02,  1.1658e-01],\n",
      "          [ 4.9400e-02,  4.7148e-02,  8.8088e-02,  4.8091e-02,  3.7375e-02],\n",
      "          [-9.9143e-02, -1.3472e-01, -1.0794e-01, -7.4328e-02, -2.5213e-02]],\n",
      "\n",
      "         [[ 1.7033e-02, -1.1509e-02, -5.9700e-02, -3.0361e-03, -4.9731e-02],\n",
      "          [-1.4444e-01, -1.4620e-02, -6.6634e-02, -3.6055e-02,  2.9875e-02],\n",
      "          [-3.0981e-02, -7.2000e-02,  6.0979e-02,  1.1676e-01, -3.7791e-03],\n",
      "          [-8.8276e-02,  1.8819e-02, -3.0833e-02,  9.4045e-02,  6.3577e-02],\n",
      "          [-8.5872e-02, -2.7445e-02,  6.7428e-02,  2.1766e-02,  1.7344e-01]],\n",
      "\n",
      "         [[-5.0242e-02,  9.3016e-02, -6.0916e-02,  1.2471e-04, -5.2089e-02],\n",
      "          [-1.5553e-02, -4.0964e-02, -9.6092e-02, -3.3929e-02, -3.0205e-02],\n",
      "          [ 4.2961e-02, -7.2261e-02, -7.8160e-03,  2.2929e-02,  8.8345e-02],\n",
      "          [-4.6669e-02,  8.5794e-03,  4.7026e-02, -6.8102e-02,  3.7116e-03],\n",
      "          [-7.4395e-02, -8.4583e-02, -1.2841e-01, -1.8408e-01, -7.4196e-02]],\n",
      "\n",
      "         [[-3.9960e-02,  2.0700e-02, -6.4154e-02, -5.5404e-02, -2.1594e-02],\n",
      "          [ 3.4590e-02, -4.2444e-02,  1.5601e-02,  8.9864e-02,  7.6000e-02],\n",
      "          [-5.7341e-02, -3.5243e-02,  6.0576e-02,  6.8400e-02, -3.4277e-02],\n",
      "          [ 1.0486e-01,  8.0053e-02,  1.0727e-02, -2.1537e-02, -1.0782e-01],\n",
      "          [-1.4862e-02, -6.2268e-02,  1.2844e-02, -4.7809e-02, -1.2332e-01]]],\n",
      "\n",
      "\n",
      "        [[[-5.9116e-02,  3.4564e-02, -8.5613e-02, -7.2749e-02, -8.5187e-02],\n",
      "          [ 4.5523e-03, -1.2325e-02,  6.3501e-02, -7.1771e-02, -2.3421e-03],\n",
      "          [ 1.6458e-01,  1.0163e-01, -5.0785e-03,  1.0077e-02,  1.1847e-01],\n",
      "          [ 1.5313e-01,  1.9613e-01,  5.9791e-02,  5.6406e-02, -2.1268e-02],\n",
      "          [ 1.8210e-03,  3.7914e-02, -4.2408e-02,  1.6149e-02, -1.6843e-02]],\n",
      "\n",
      "         [[ 3.7828e-03,  1.7824e-02,  1.0655e-01,  1.3131e-02,  3.5184e-02],\n",
      "          [-1.1224e-01,  5.9485e-02,  4.6388e-02, -3.6729e-02,  4.2256e-02],\n",
      "          [-1.1362e-01, -8.8868e-02, -1.1994e-02,  1.3842e-02,  6.9733e-02],\n",
      "          [ 9.2603e-03,  3.0135e-02,  7.5017e-02, -1.3312e-01, -1.2747e-01],\n",
      "          [-7.6477e-02, -6.3389e-02,  4.9380e-02,  9.0430e-02,  2.2772e-02]],\n",
      "\n",
      "         [[-7.1105e-03, -5.6237e-02,  2.8493e-03,  3.6038e-02, -6.8018e-02],\n",
      "          [ 7.1266e-03,  5.7182e-02, -4.8677e-02,  4.1755e-02,  9.2942e-02],\n",
      "          [ 1.0895e-02, -4.1852e-02,  3.8123e-02,  6.7030e-02,  1.6479e-01],\n",
      "          [-2.7235e-02,  3.3534e-02,  1.1197e-01,  1.7514e-02,  9.5900e-02],\n",
      "          [-5.8891e-02, -3.6122e-02,  8.7792e-02,  2.3124e-02, -2.5444e-02]],\n",
      "\n",
      "         [[-1.7770e-01, -1.0453e-02,  1.0170e-01, -8.9903e-02,  6.0790e-02],\n",
      "          [-1.0544e-02, -2.6796e-02,  7.6661e-02, -1.7165e-02, -7.1832e-02],\n",
      "          [-1.4602e-01,  2.2504e-02, -2.6469e-02, -8.0847e-02,  1.8840e-02],\n",
      "          [ 2.5265e-02,  9.9329e-02,  2.0184e-02,  8.4443e-02,  1.5386e-01],\n",
      "          [ 5.7790e-02,  1.4116e-01,  2.5445e-01,  1.0991e-01,  2.1543e-01]],\n",
      "\n",
      "         [[-6.1246e-02,  1.4569e-02, -2.2759e-02,  6.2312e-03,  7.0244e-02],\n",
      "          [ 5.1868e-02, -3.7805e-02, -2.8733e-02,  4.5566e-02, -4.1273e-02],\n",
      "          [-5.5831e-02, -2.2132e-02, -2.2801e-02, -6.2694e-03, -9.7611e-02],\n",
      "          [ 7.3627e-03, -3.5916e-02,  1.8825e-02, -1.5076e-01, -1.1613e-01],\n",
      "          [-4.7395e-02,  5.5510e-02, -5.3115e-02, -1.9187e-02, -5.5349e-02]],\n",
      "\n",
      "         [[ 8.2287e-02,  2.5060e-02, -9.1896e-02,  1.1048e-01,  9.8831e-02],\n",
      "          [ 4.9240e-02, -7.1275e-02,  3.5032e-02, -4.9667e-02, -7.6443e-02],\n",
      "          [-9.6600e-03, -1.0801e-01, -9.4620e-02, -1.2874e-01, -3.2005e-02],\n",
      "          [-1.1614e-01,  2.4326e-02, -1.3038e-02,  6.1589e-04, -2.1011e-02],\n",
      "          [ 3.0382e-02,  2.3326e-02,  1.1312e-02, -8.1869e-02,  4.8298e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.2127e-02, -5.9811e-02, -3.7400e-02, -4.2041e-02,  4.2239e-02],\n",
      "          [ 2.9065e-02, -1.9066e-02, -1.2779e-01, -3.9764e-02,  1.2348e-01],\n",
      "          [ 1.9044e-02,  1.2281e-02,  8.8485e-02, -1.1672e-02, -3.6006e-02],\n",
      "          [ 3.6586e-02,  1.4287e-01,  6.5647e-02,  1.0241e-01,  3.6836e-02],\n",
      "          [-4.7432e-02,  2.2898e-02,  1.1642e-01,  6.5317e-02,  2.2756e-02]],\n",
      "\n",
      "         [[-3.0275e-02, -4.4953e-02,  4.6554e-02, -5.4886e-02, -9.7015e-03],\n",
      "          [-9.8128e-02, -2.6940e-02,  4.9789e-02, -1.1050e-01, -3.1516e-02],\n",
      "          [ 2.0490e-02, -8.3490e-02,  4.0399e-02, -6.5635e-02, -6.9236e-02],\n",
      "          [ 4.6732e-03, -1.1073e-01, -7.9099e-02,  5.6986e-02,  4.7234e-02],\n",
      "          [ 2.8334e-02,  1.6409e-02, -1.0889e-01, -1.9558e-02,  4.5183e-02]],\n",
      "\n",
      "         [[-6.0202e-02, -3.8869e-02, -1.4605e-02,  1.1900e-01, -7.7430e-02],\n",
      "          [ 2.7412e-02, -8.3792e-02, -1.7612e-01, -1.8621e-02,  2.5981e-03],\n",
      "          [-8.0723e-02, -4.5357e-02,  1.9811e-02,  4.8289e-02, -3.0540e-02],\n",
      "          [-3.7920e-02, -9.1071e-02, -2.8149e-02,  1.1856e-02, -7.6995e-02],\n",
      "          [ 1.7433e-02,  1.4300e-02, -8.9354e-02, -2.5740e-02, -7.0674e-02]],\n",
      "\n",
      "         [[-1.0321e-02,  2.2217e-02,  5.8344e-02,  1.2004e-01,  1.1483e-01],\n",
      "          [-3.3907e-02, -5.2645e-02, -1.0275e-01, -3.3033e-02,  1.3237e-01],\n",
      "          [ 3.2108e-02, -9.9741e-02, -4.4214e-01, -1.3854e-01,  2.5844e-02],\n",
      "          [ 1.1188e-01, -7.3207e-02, -2.3412e-01, -1.1385e-01,  5.1503e-02],\n",
      "          [ 1.1375e-01,  6.1476e-02, -1.6320e-01, -1.0386e-01, -7.9806e-02]],\n",
      "\n",
      "         [[ 4.4369e-02,  1.1196e-01,  8.8183e-02,  8.5561e-02,  2.3736e-02],\n",
      "          [-8.1872e-03,  1.8869e-02,  5.2736e-03,  8.3611e-02,  1.8391e-02],\n",
      "          [ 2.7170e-02, -3.8346e-02,  1.0942e-02,  4.8930e-02, -6.6865e-02],\n",
      "          [ 8.2243e-02,  6.2065e-02,  6.2325e-02,  4.5160e-02, -9.3304e-04],\n",
      "          [ 3.1411e-02,  3.3572e-02,  5.8376e-02,  5.3547e-02,  1.3024e-02]],\n",
      "\n",
      "         [[ 4.6797e-02,  2.9005e-02,  1.4205e-02,  8.3565e-02, -6.1304e-02],\n",
      "          [-4.6490e-02, -2.3575e-02,  1.3286e-01,  1.6733e-01, -1.6580e-02],\n",
      "          [-1.6601e-03, -1.5829e-02, -8.3669e-02,  3.5517e-02,  4.4429e-02],\n",
      "          [-3.0874e-02, -9.9889e-02, -1.4253e-01, -1.6821e-02,  8.0190e-02],\n",
      "          [ 1.5917e-02, -1.1171e-01, -1.0504e-02, -5.0021e-02,  3.5743e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 2.2509e-02, -4.1858e-02,  9.4795e-03, -1.1653e-01,  3.6478e-02],\n",
      "          [-5.1155e-02, -1.1557e-01, -1.3740e-01,  8.7531e-03,  1.9670e-02],\n",
      "          [-8.5855e-02, -2.3020e-01,  2.7491e-02, -2.4279e-02,  5.6568e-02],\n",
      "          [-1.2836e-01, -4.5076e-02, -1.3153e-02,  1.7350e-02,  4.6486e-02],\n",
      "          [-1.2617e-01, -2.8820e-03,  2.2654e-03, -5.3205e-02,  8.6835e-03]],\n",
      "\n",
      "         [[-5.2566e-02, -7.5762e-02,  1.0305e-02, -4.1468e-02,  3.8596e-02],\n",
      "          [-2.9012e-02, -6.6946e-02,  1.1234e-02, -3.0786e-02, -4.8590e-02],\n",
      "          [-1.5244e-01, -2.8921e-02, -7.4011e-02, -6.4682e-02, -2.4730e-03],\n",
      "          [-9.5106e-02, -1.1439e-01, -1.2584e-01, -6.1297e-02,  1.4644e-02],\n",
      "          [-1.0649e-01, -1.4882e-01, -1.2535e-01,  4.5662e-02,  2.1020e-02]],\n",
      "\n",
      "         [[-5.1014e-02,  3.7534e-02,  2.2976e-02,  1.4798e-02, -1.3465e-02],\n",
      "          [ 2.3101e-02,  9.7934e-02,  1.0510e-01,  8.8688e-02, -3.4358e-02],\n",
      "          [ 6.1506e-02,  9.2267e-02,  8.1102e-02, -7.3178e-02,  3.2546e-02],\n",
      "          [-1.2430e-02, -8.6893e-02,  8.3978e-02, -1.5532e-02,  3.7283e-02],\n",
      "          [-2.0909e-02,  3.9866e-02,  3.4207e-02,  2.8253e-02,  3.5983e-02]],\n",
      "\n",
      "         [[ 6.3072e-02, -2.4401e-02, -4.6455e-02, -9.4513e-04, -7.2703e-02],\n",
      "          [ 1.2528e-01,  8.2809e-02, -4.8432e-02, -9.7398e-02, -5.4404e-02],\n",
      "          [ 6.1007e-02, -9.9897e-02, -5.1334e-02, -1.3002e-01,  3.5029e-03],\n",
      "          [-3.8181e-02, -2.8253e-01, -2.2149e-01, -1.0135e-01,  9.1318e-03],\n",
      "          [-1.8895e-02, -2.6293e-01, -1.9624e-01, -5.6792e-02, -1.5241e-03]],\n",
      "\n",
      "         [[ 1.2218e-02,  8.9081e-02,  2.5156e-02, -4.0462e-02, -3.8575e-02],\n",
      "          [-2.4620e-02,  5.1580e-02,  1.9392e-02, -7.0331e-02, -6.4092e-02],\n",
      "          [ 3.8575e-03,  6.3599e-03,  6.0151e-03,  6.4734e-02, -1.3669e-02],\n",
      "          [ 1.3753e-01, -2.9150e-02,  6.4836e-02, -4.4512e-02,  6.3437e-02],\n",
      "          [ 1.4187e-01, -6.1012e-02, -8.2146e-02, -5.9884e-02,  1.3212e-01]],\n",
      "\n",
      "         [[-9.8267e-02, -1.7923e-02, -2.5584e-03,  6.2860e-02,  8.8637e-02],\n",
      "          [-9.1174e-02, -5.1191e-03,  7.8674e-02,  8.6380e-02, -4.5604e-02],\n",
      "          [-8.2257e-02,  2.6995e-02,  1.5451e-01, -3.6305e-02, -3.9767e-02],\n",
      "          [-8.6366e-02,  7.8500e-03,  2.8129e-02,  1.7544e-02,  3.7108e-03],\n",
      "          [-1.4034e-01, -4.7788e-02,  5.3831e-02,  7.9794e-02,  6.2750e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.3719e-02,  3.0753e-02, -2.5528e-02, -5.2303e-02, -1.1331e-01],\n",
      "          [ 1.1513e-02,  5.1030e-02, -5.0406e-02, -1.9130e-01, -5.7812e-02],\n",
      "          [ 1.6838e-02,  1.4407e-04, -9.9829e-02, -7.5436e-02, -7.3844e-02],\n",
      "          [-6.2521e-02,  2.4864e-02, -6.6871e-02, -7.7935e-02, -1.4438e-01],\n",
      "          [-5.7435e-02, -1.6651e-01, -2.1573e-01, -1.2270e-01, -2.7544e-02]],\n",
      "\n",
      "         [[ 1.8635e-02,  1.0796e-02, -4.4872e-02, -6.3879e-02, -5.4276e-02],\n",
      "          [-1.4365e-03,  4.6635e-02,  7.9841e-03, -7.6030e-04,  5.0013e-03],\n",
      "          [ 3.7114e-02, -1.3719e-02,  7.1457e-02,  3.6620e-02,  5.2360e-02],\n",
      "          [-6.6112e-02, -3.8761e-02,  1.1161e-01,  8.0670e-02, -1.0222e-02],\n",
      "          [ 2.7728e-02,  5.2410e-02,  7.2471e-02, -5.8913e-02, -8.9759e-02]],\n",
      "\n",
      "         [[ 9.7348e-02,  2.2319e-02,  8.8660e-02,  6.7401e-02,  4.0693e-02],\n",
      "          [ 9.8410e-02,  2.8925e-02,  1.0841e-01,  1.3000e-01,  7.8668e-02],\n",
      "          [ 8.8904e-02, -4.2277e-02, -5.4109e-02,  2.9210e-02,  1.3105e-01],\n",
      "          [-1.7075e-02, -4.6264e-02,  7.2689e-02,  1.4449e-02, -4.8872e-02],\n",
      "          [ 1.1608e-01,  5.4153e-02, -5.1774e-02,  6.5077e-02,  2.1882e-02]],\n",
      "\n",
      "         [[ 3.5126e-02,  5.0352e-02,  3.3235e-02, -6.0850e-03,  6.4920e-02],\n",
      "          [ 1.0924e-01,  9.0090e-02,  1.9811e-01,  9.2714e-02,  1.0345e-01],\n",
      "          [ 3.5683e-02, -3.8480e-02,  1.1590e-01, -5.8137e-03, -5.7410e-02],\n",
      "          [ 3.4219e-03,  6.6345e-03,  1.3835e-01, -1.0939e-01, -2.5476e-03],\n",
      "          [-3.8663e-02,  1.7829e-01,  7.9684e-02, -1.0591e-01, -6.4718e-02]],\n",
      "\n",
      "         [[ 6.6293e-02,  7.6153e-02,  1.7969e-01,  9.8652e-02,  8.8752e-02],\n",
      "          [-1.5760e-02,  5.7230e-02, -5.0821e-03,  9.0623e-02, -1.7771e-02],\n",
      "          [-1.7934e-02, -1.4052e-02, -5.0060e-02, -3.4541e-02,  6.8171e-02],\n",
      "          [ 2.8425e-02,  2.7972e-02,  2.1323e-02, -1.5761e-02, -1.6301e-03],\n",
      "          [ 1.2946e-02, -6.5432e-03, -4.6381e-02, -1.1243e-01, -6.2400e-02]],\n",
      "\n",
      "         [[-9.4761e-02, -6.6049e-02, -2.5712e-02, -4.8261e-02, -1.0969e-01],\n",
      "          [-1.0561e-01, -7.7204e-02, -3.7886e-03, -1.6943e-02,  2.6633e-02],\n",
      "          [-1.1173e-01, -8.0623e-02, -4.0922e-02,  9.2584e-02,  2.4860e-02],\n",
      "          [-6.9378e-02, -1.4561e-01,  5.8563e-03,  1.9044e-02,  7.5119e-02],\n",
      "          [ 2.6484e-02, -7.0713e-02,  7.0723e-02,  8.0775e-02,  2.1901e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.3026e-02, -9.3306e-02, -8.1003e-02,  3.2301e-02, -6.3004e-02],\n",
      "          [-1.1271e-01, -9.3377e-02, -2.1497e-02, -4.4186e-02, -4.3075e-02],\n",
      "          [-1.2156e-01,  5.2279e-02, -4.6254e-02,  4.5020e-02, -1.3959e-02],\n",
      "          [ 3.8935e-03,  2.6573e-02,  2.9784e-02,  3.8078e-05, -4.2707e-02],\n",
      "          [ 5.4925e-04,  2.7618e-02,  1.1400e-01,  1.0682e-01,  2.3517e-02]],\n",
      "\n",
      "         [[ 3.3303e-02,  4.0655e-03,  6.1452e-02,  1.1755e-01, -4.7786e-02],\n",
      "          [ 8.0966e-02, -5.2158e-02, -5.2991e-02,  3.0049e-02,  3.4166e-02],\n",
      "          [-7.4337e-02, -1.4302e-01,  7.3752e-02, -5.0325e-02, -1.8469e-02],\n",
      "          [-8.3824e-02, -3.1246e-02,  5.4726e-02, -5.7535e-02,  4.8182e-02],\n",
      "          [ 1.1297e-02,  4.9738e-02,  1.0760e-01,  2.8081e-02,  2.1323e-02]],\n",
      "\n",
      "         [[-7.5452e-03,  7.3466e-02,  1.0578e-01,  9.5160e-02,  1.7289e-03],\n",
      "          [ 6.5814e-02,  4.8662e-02,  2.8566e-02, -4.5828e-02, -1.8599e-02],\n",
      "          [-4.3475e-02, -1.1679e-01, -1.1823e-01, -1.6578e-01, -6.0711e-02],\n",
      "          [ 7.4456e-03, -6.2664e-02, -2.9107e-02,  2.7554e-02,  4.5589e-02],\n",
      "          [ 5.0262e-02,  8.1630e-02,  1.0712e-01,  1.0568e-01,  7.7697e-02]],\n",
      "\n",
      "         [[ 5.2478e-02,  4.3791e-02,  8.5821e-02,  2.9371e-02,  7.5293e-02],\n",
      "          [-3.8071e-02,  1.4790e-01,  1.4473e-01, -1.7673e-02, -1.2317e-02],\n",
      "          [ 2.3790e-03, -1.2424e-01, -1.5226e-01, -1.0195e-01,  1.0035e-02],\n",
      "          [-1.3320e-01, -1.2621e-01, -5.0725e-02, -5.6134e-02,  2.0978e-02],\n",
      "          [ 4.1760e-03, -7.8735e-02, -1.0016e-01, -6.0463e-02,  5.7443e-02]],\n",
      "\n",
      "         [[-2.7715e-02,  2.5409e-02,  1.2778e-01,  7.0754e-02,  7.8404e-02],\n",
      "          [ 9.6741e-02,  1.6020e-01,  8.8225e-02,  5.0991e-02,  1.4563e-01],\n",
      "          [ 3.0024e-02, -1.4646e-02,  1.6815e-02, -4.7639e-02, -5.4477e-02],\n",
      "          [-6.2172e-02, -5.4761e-02, -1.5956e-01, -1.3988e-01, -1.2670e-01],\n",
      "          [-4.4475e-02, -1.3830e-01, -2.3151e-02, -4.2630e-02, -5.9431e-02]],\n",
      "\n",
      "         [[-7.9523e-02,  2.4667e-02,  2.2837e-02, -2.2834e-02,  5.0297e-02],\n",
      "          [-5.7216e-02, -9.8623e-03,  1.1193e-01,  1.3062e-02,  3.1667e-03],\n",
      "          [-6.7534e-02,  6.0704e-02, -8.8612e-02,  7.2142e-02, -9.9839e-02],\n",
      "          [ 1.6851e-02,  2.8716e-02,  4.3481e-02,  6.2678e-02, -8.0339e-02],\n",
      "          [ 5.0232e-02,  1.0986e-01, -1.2276e-02, -9.8270e-02, -1.1276e-02]]]])), ('layer2.0.bias', tensor([ 0.0495, -0.0958,  0.0302,  0.0224,  0.0183, -0.0747,  0.0476,  0.0228,\n",
      "        -0.0122,  0.0126,  0.0650, -0.0678,  0.0624, -0.0136, -0.0210,  0.0653])), ('layer2.1.weight', tensor([1.0839, 1.0972, 1.1747, 1.1199, 1.0609, 1.1082, 1.1690, 1.0992, 1.0900,\n",
      "        1.0109, 1.2804, 1.1893, 1.0892, 1.1995, 1.0682, 1.1351])), ('layer2.1.bias', tensor([-0.0784, -0.1578, -0.1859, -0.0486, -0.1657, -0.0969, -0.1554, -0.1212,\n",
      "        -0.0455, -0.1699, -0.0920, -0.0759, -0.0434, -0.1548, -0.1070, -0.1018])), ('layer2.1.running_mean', tensor([-0.0637,  0.3608, -0.3524,  0.5613, -0.3195, -0.1242,  0.3390,  1.7665,\n",
      "        -0.5449, -0.0135,  0.0024,  0.3556, -1.6666, -1.4490, -0.1245, -0.1062])), ('layer2.1.running_var', tensor([0.9500, 0.5956, 0.6259, 0.9644, 0.4605, 0.2607, 0.8792, 0.6409, 1.1536,\n",
      "        0.4991, 0.4896, 0.4082, 1.0560, 0.3891, 0.7945, 0.4853])), ('layer2.1.num_batches_tracked', tensor(3600)), ('fc1.weight', tensor([[ 0.0143, -0.0808, -0.0383,  ..., -0.0246, -0.0702, -0.0905],\n",
      "        [-0.0412, -0.0186,  0.0116,  ..., -0.0180, -0.0070, -0.0529],\n",
      "        [ 0.0270, -0.0626,  0.0154,  ..., -0.0231,  0.0849, -0.0412],\n",
      "        ...,\n",
      "        [-0.0240,  0.0148, -0.0238,  ...,  0.0698,  0.0160, -0.0427],\n",
      "        [ 0.0345, -0.0329,  0.0098,  ...,  0.0401,  0.0433, -0.0399],\n",
      "        [ 0.0133, -0.0334,  0.0427,  ..., -0.1328, -0.1069,  0.0631]])), ('fc1.bias', tensor([-0.0155,  0.0532,  0.0217,  0.0712,  0.0067,  0.0080, -0.0484,  0.0258,\n",
      "         0.0285,  0.0270,  0.0418, -0.0031, -0.0415, -0.0349,  0.0027, -0.0415,\n",
      "         0.0801,  0.0596, -0.0317, -0.0180,  0.0313, -0.0295, -0.0404, -0.0002,\n",
      "         0.0562, -0.0028,  0.0153,  0.0522,  0.0478, -0.0026,  0.0848, -0.0549,\n",
      "         0.0167, -0.0050,  0.0247, -0.0676,  0.0484, -0.0110,  0.0053, -0.0020,\n",
      "         0.0062,  0.0600,  0.0287,  0.0266,  0.0366,  0.0004, -0.0200,  0.0337,\n",
      "        -0.0223, -0.0041,  0.0038,  0.0069,  0.0330,  0.0610, -0.0108, -0.0461,\n",
      "        -0.0084, -0.0065, -0.0205, -0.0093, -0.0377, -0.0329,  0.0396,  0.0232,\n",
      "        -0.0216, -0.0275, -0.0052,  0.0401, -0.0290,  0.0413,  0.0194,  0.0346,\n",
      "        -0.0182,  0.0422,  0.0298, -0.0099,  0.0475,  0.0283,  0.0252, -0.0066,\n",
      "         0.0523,  0.0340, -0.0502,  0.0426,  0.0084, -0.0392,  0.0033,  0.0080,\n",
      "        -0.0175, -0.0116,  0.0067, -0.0112,  0.0310,  0.0098,  0.0362,  0.0268,\n",
      "         0.0346, -0.0292, -0.0001, -0.0326, -0.0023, -0.0081, -0.0036,  0.0161,\n",
      "         0.0244,  0.0394, -0.0570, -0.0713, -0.0520,  0.0109,  0.0063,  0.0190,\n",
      "        -0.0358,  0.0029, -0.0485,  0.0057,  0.0029,  0.0474, -0.0408, -0.0017])), ('fc2.weight', tensor([[-0.0633,  0.0178,  0.0613,  ...,  0.0346,  0.0162,  0.0443],\n",
      "        [-0.0147,  0.0038,  0.0214,  ..., -0.0846,  0.0075,  0.1503],\n",
      "        [-0.0198, -0.0063,  0.0879,  ..., -0.0127, -0.0483, -0.0637],\n",
      "        ...,\n",
      "        [-0.0170, -0.0472,  0.0488,  ..., -0.0757, -0.0485, -0.0666],\n",
      "        [-0.0286,  0.1229,  0.1074,  ...,  0.1084, -0.0141, -0.0675],\n",
      "        [-0.0627,  0.0299,  0.1819,  ...,  0.0762,  0.0907, -0.0364]])), ('fc2.bias', tensor([-0.0165,  0.0194,  0.0864, -0.0316,  0.0740,  0.0604,  0.0146, -0.0155,\n",
      "         0.0210, -0.0156, -0.0906, -0.0018, -0.0411,  0.1166,  0.0057, -0.0604,\n",
      "         0.0052, -0.0793, -0.0090, -0.0455,  0.0567, -0.0127,  0.1088, -0.1026,\n",
      "         0.0480,  0.0768, -0.0445, -0.0669,  0.0472, -0.0762,  0.1328,  0.0422,\n",
      "         0.0448,  0.0583, -0.0516,  0.0571,  0.0438, -0.0252,  0.0127, -0.1019,\n",
      "        -0.0827,  0.0674,  0.0552,  0.1244,  0.0164, -0.0704, -0.0367,  0.0096,\n",
      "         0.0186,  0.0786, -0.0851,  0.0019, -0.0318,  0.0304, -0.0530,  0.0474,\n",
      "         0.0114,  0.0539, -0.0871, -0.0272,  0.0730,  0.0720,  0.0721,  0.0603,\n",
      "        -0.0245,  0.0839, -0.0668,  0.0534,  0.0506, -0.0235, -0.0123, -0.0590,\n",
      "         0.1047,  0.0688,  0.0384,  0.0416, -0.0640,  0.0269,  0.0388, -0.0533,\n",
      "        -0.0847, -0.0523, -0.0118,  0.0137])), ('fc3.weight', tensor([[-1.7206e-01, -2.6733e-02,  1.6724e-01,  1.6661e-01,  1.3674e-01,\n",
      "          1.2412e-01, -2.8072e-01, -7.0449e-02,  3.2783e-03, -3.8244e-02,\n",
      "          8.0867e-03, -1.5574e-01, -1.0489e-01, -1.4378e-01,  9.0352e-02,\n",
      "          4.4841e-02,  1.3685e-02,  8.1515e-02, -1.1523e-01,  4.3876e-02,\n",
      "          3.7148e-02,  1.1046e-01, -1.7864e-01,  7.9444e-02, -9.1255e-02,\n",
      "         -1.9719e-01,  3.1821e-02,  1.0944e-02, -2.0081e-01, -7.8703e-02,\n",
      "         -1.1369e-01,  1.5897e-01,  1.3341e-01,  9.4381e-02, -1.0787e-02,\n",
      "          9.3593e-02,  1.3146e-01, -2.8642e-03, -7.3163e-02,  6.8784e-02,\n",
      "          2.5484e-02, -1.5076e-01, -1.8775e-01,  2.3924e-02, -2.7593e-02,\n",
      "         -5.8263e-02, -1.1714e-03, -6.2614e-02, -1.8633e-01, -1.1337e-01,\n",
      "          6.7614e-02,  9.3469e-02, -4.3277e-02, -1.1977e-01, -4.0150e-02,\n",
      "         -7.4201e-02,  8.7978e-02, -3.2427e-02, -8.7848e-02,  1.0316e-01,\n",
      "         -6.0550e-02, -6.2919e-02, -1.5723e-01, -3.4023e-02,  3.3758e-03,\n",
      "          1.3756e-01, -1.6896e-01, -1.5057e-01,  5.3366e-02, -1.2463e-01,\n",
      "          1.4827e-02, -1.0578e-01, -7.6063e-02,  7.0114e-02,  2.7187e-02,\n",
      "         -1.7358e-01, -1.9875e-01, -4.8220e-02, -6.1634e-02, -4.7885e-02,\n",
      "          1.0200e-01, -4.8768e-02,  6.8811e-02,  2.6543e-02],\n",
      "        [-4.7832e-02,  5.1317e-02,  1.2976e-01,  1.1788e-01,  4.2950e-02,\n",
      "         -1.3151e-01, -1.0592e-02, -9.8349e-02,  1.2034e-01, -1.7969e-01,\n",
      "          9.5596e-02, -5.5192e-02, -1.0335e-01, -3.6332e-02, -1.0365e-01,\n",
      "         -3.9631e-02,  8.3228e-02, -5.3714e-02, -5.1154e-02, -2.1322e-01,\n",
      "          4.0798e-02, -4.2168e-02,  1.8955e-01, -1.0026e-01,  7.5254e-02,\n",
      "          1.1035e-01,  1.2119e-01, -4.1386e-02, -9.6204e-03, -5.0591e-02,\n",
      "          1.0775e-01,  1.8447e-02, -1.3739e-01,  5.1959e-02,  1.0542e-01,\n",
      "         -6.5357e-02,  1.4445e-01, -2.0374e-01,  1.0958e-01, -8.7783e-02,\n",
      "         -2.0815e-02, -1.1136e-02,  5.8146e-02,  1.1823e-01, -8.2321e-02,\n",
      "         -1.0687e-01,  9.4195e-02, -3.3487e-02,  1.5298e-01,  1.2308e-01,\n",
      "         -1.4169e-01,  3.7557e-02, -2.1374e-01, -1.9604e-01, -9.6146e-02,\n",
      "          1.1793e-01, -1.9553e-01, -2.1330e-01, -2.0349e-01, -2.1548e-01,\n",
      "         -1.4644e-01, -4.0172e-02, -1.3655e-01,  1.0824e-02, -2.1693e-01,\n",
      "         -3.8600e-02, -8.0192e-02, -5.0377e-03, -3.5348e-02,  8.8257e-02,\n",
      "         -4.6880e-02,  2.7240e-02,  1.1287e-01, -1.6293e-01, -6.8121e-02,\n",
      "          1.2966e-01, -9.1563e-02,  7.6865e-02, -6.8430e-03, -1.9060e-02,\n",
      "         -7.7834e-02,  6.9265e-02,  1.0243e-01,  1.4655e-01],\n",
      "        [-1.2649e-01, -9.0758e-02, -6.9073e-02, -1.4106e-01, -1.4142e-01,\n",
      "         -6.2715e-02,  1.4505e-01,  9.5404e-02,  5.4847e-02, -1.6091e-01,\n",
      "         -5.8025e-02, -1.3136e-01, -9.7447e-02, -6.5237e-02, -3.0421e-03,\n",
      "          1.4103e-01,  1.2040e-01, -6.4314e-02,  6.0602e-02, -9.0328e-02,\n",
      "         -6.3233e-02,  1.2660e-01,  8.5494e-02, -8.8129e-02,  1.5524e-01,\n",
      "         -1.1407e-01,  1.1578e-01, -5.1023e-02, -1.4332e-01,  1.0892e-02,\n",
      "         -1.3980e-01,  1.3358e-01,  1.5438e-01,  7.5884e-02, -3.7768e-02,\n",
      "         -4.7911e-02, -7.5498e-02, -1.9546e-02,  4.3175e-02,  1.0173e-01,\n",
      "          3.1531e-02,  1.1859e-01,  2.5828e-02,  1.5788e-01, -8.2326e-02,\n",
      "          6.7524e-02, -5.4142e-02,  9.5348e-02, -3.9955e-02, -3.1033e-02,\n",
      "         -2.4998e-01, -8.5666e-02,  1.0980e-01, -2.0779e-02,  2.2396e-02,\n",
      "          3.5217e-02, -7.9093e-02,  1.2237e-01, -1.2619e-02, -1.6337e-01,\n",
      "         -3.0531e-02, -1.4643e-01, -7.3219e-02, -2.8017e-02,  4.2486e-02,\n",
      "          8.6845e-02,  1.1911e-01, -6.5974e-02,  1.0502e-01, -1.2783e-02,\n",
      "          9.6763e-02,  3.5683e-02, -8.0945e-02,  1.0142e-01,  4.3463e-02,\n",
      "         -2.0877e-02,  1.3626e-01,  9.6184e-03, -7.0711e-02,  5.6808e-02,\n",
      "          4.8181e-02,  4.6437e-02, -1.5178e-01, -9.2111e-02],\n",
      "        [ 1.3305e-01, -3.3015e-02, -1.9219e-01, -2.5144e-02, -1.7693e-01,\n",
      "         -8.9964e-02,  1.1321e-01, -2.4571e-02,  7.3617e-02,  4.8968e-03,\n",
      "          1.9947e-02, -8.7945e-02, -5.1936e-02,  7.8210e-03,  9.2657e-02,\n",
      "          1.5677e-02, -1.6394e-01,  6.2098e-02, -6.5290e-02, -3.2103e-02,\n",
      "          1.8968e-02, -7.4751e-02,  1.1156e-01, -1.4499e-01,  1.6627e-02,\n",
      "          1.1899e-01, -2.9175e-02, -3.8601e-02, -1.4501e-01, -5.0929e-02,\n",
      "         -1.4538e-01, -6.2007e-02, -1.7833e-01,  6.8535e-02, -1.2223e-01,\n",
      "          1.0378e-01, -1.0040e-01,  7.5904e-02, -7.8530e-02,  1.5155e-02,\n",
      "          8.3925e-02,  1.0971e-01,  1.0250e-01, -1.6316e-01,  7.9747e-02,\n",
      "          5.4614e-02,  1.4043e-01,  9.9949e-02, -9.1643e-02, -1.1967e-01,\n",
      "         -4.1486e-02, -1.9386e-02, -1.0928e-01, -1.1243e-01, -6.9007e-02,\n",
      "         -2.6569e-01,  2.1897e-02, -1.1199e-01,  7.3009e-02,  5.6194e-02,\n",
      "         -1.4434e-01, -1.5159e-01,  6.8195e-02, -1.6122e-01, -9.3668e-02,\n",
      "         -2.2464e-02, -1.3645e-01,  1.5037e-01,  7.8050e-02,  1.0769e-02,\n",
      "         -2.2368e-04,  3.4997e-02, -1.3189e-01,  7.1151e-02,  9.3588e-02,\n",
      "          1.2256e-01,  1.0429e-01, -9.0988e-02,  1.8355e-01,  5.7405e-02,\n",
      "          1.2534e-01,  2.9031e-02,  7.1020e-02,  2.2906e-02],\n",
      "        [-2.2944e-01,  1.0903e-01, -2.2433e-02, -2.2429e-01,  8.3310e-02,\n",
      "         -8.3328e-02, -1.7874e-01,  1.1305e-02,  7.4818e-02, -1.8008e-01,\n",
      "          7.0541e-02,  7.8365e-02,  5.9550e-02, -4.3930e-02, -2.0235e-01,\n",
      "         -1.6046e-01, -3.1637e-02, -7.4144e-02, -9.4334e-03, -1.7558e-01,\n",
      "         -3.9822e-02, -7.5060e-03, -4.5948e-03, -4.2105e-02,  1.8067e-01,\n",
      "          9.7001e-02, -1.0955e-01,  1.1092e-01,  1.1345e-01,  7.6121e-02,\n",
      "         -8.8177e-02,  1.3843e-02,  3.0420e-02,  5.5810e-02, -8.0216e-02,\n",
      "          7.3109e-02, -9.0429e-02,  8.5697e-02,  4.1152e-03, -1.1278e-02,\n",
      "         -6.2211e-02,  8.7141e-03, -1.4488e-02,  6.3036e-02, -2.0163e-01,\n",
      "          7.6697e-02, -1.2192e-01, -1.0971e-01,  7.6536e-02, -5.8471e-02,\n",
      "         -3.2153e-02,  9.6798e-02, -3.0215e-02,  8.0108e-02,  2.9291e-02,\n",
      "          1.0311e-01, -6.5458e-02, -1.5820e-02,  1.4388e-01,  1.5181e-01,\n",
      "          1.0185e-01, -9.3988e-02,  1.1922e-01,  1.7528e-02,  1.7516e-01,\n",
      "         -9.6583e-02,  4.1378e-02,  2.9559e-02,  7.7693e-02, -1.9170e-01,\n",
      "         -1.5435e-01, -3.4674e-02,  1.5226e-01, -2.2901e-01, -1.1278e-01,\n",
      "          9.7109e-02, -2.2319e-01, -2.1443e-02,  8.2201e-02, -5.7619e-02,\n",
      "          5.0037e-02,  7.2523e-02, -2.2206e-01,  3.3937e-02],\n",
      "        [ 6.5759e-02, -1.5145e-01, -9.9463e-02, -5.1391e-02,  5.0720e-02,\n",
      "          6.1016e-02, -7.5123e-02,  6.2937e-02,  5.9095e-02,  1.0249e-01,\n",
      "         -1.1620e-02, -8.0838e-02,  1.7784e-01,  8.3309e-02,  9.9333e-02,\n",
      "          5.5050e-03, -1.0464e-01,  9.9635e-02, -9.9159e-02,  9.3443e-02,\n",
      "          6.8881e-03, -1.4734e-01, -4.4469e-02, -7.3568e-02,  1.2500e-01,\n",
      "          1.0678e-01, -1.4315e-02,  9.2238e-02, -1.3490e-01, -7.0513e-02,\n",
      "         -6.3022e-02, -1.3926e-02,  4.4459e-02, -6.4167e-02, -1.1733e-01,\n",
      "          1.0540e-01,  2.6596e-02,  1.4429e-01,  9.7207e-02, -8.5073e-02,\n",
      "          1.3116e-02, -1.3489e-01, -1.3030e-01, -1.2659e-01, -2.6475e-02,\n",
      "          1.5249e-01, -4.2572e-02, -1.7189e-01,  8.6915e-02,  2.1237e-02,\n",
      "         -1.5946e-01,  4.8646e-02, -2.0169e-01,  1.1158e-01,  9.4552e-02,\n",
      "         -5.7553e-02, -8.0304e-02, -5.8517e-02, -1.2497e-01, -2.5395e-02,\n",
      "          7.3048e-02, -2.1285e-03, -1.0119e-01,  4.3740e-02, -1.8885e-01,\n",
      "         -1.7284e-01,  5.7712e-02,  1.5370e-01, -5.4618e-02, -4.1034e-02,\n",
      "          9.6158e-02, -5.0691e-02, -2.5463e-02,  8.1352e-02, -1.3640e-01,\n",
      "         -4.6369e-02, -1.1748e-01,  1.2495e-01,  3.0347e-03,  5.2533e-02,\n",
      "          2.2325e-02, -4.7181e-03,  7.5423e-02, -1.4815e-01],\n",
      "        [-2.6274e-02, -1.7715e-01, -1.2024e-01,  5.5793e-02,  1.0834e-01,\n",
      "          9.1191e-02, -1.3107e-01, -9.0940e-02, -4.7762e-02,  1.0302e-01,\n",
      "         -2.8848e-02, -1.4634e-01,  1.5705e-01, -2.4033e-01,  1.5212e-01,\n",
      "          4.6217e-02,  4.8758e-02, -5.5826e-02, -1.3400e-01, -1.4740e-02,\n",
      "         -4.3820e-02,  3.4078e-02,  7.1728e-02,  7.6412e-02,  8.0309e-02,\n",
      "         -1.3361e-01,  1.2326e-01, -9.6706e-02, -1.4023e-02, -6.8538e-02,\n",
      "          1.7840e-02, -1.4057e-01,  4.6968e-02,  2.1120e-02, -1.2203e-01,\n",
      "         -1.6296e-01, -7.9053e-02,  2.3630e-02, -1.4116e-01, -3.7731e-02,\n",
      "         -7.9365e-02, -1.3881e-01, -3.5161e-02, -1.2781e-01, -1.5392e-01,\n",
      "          7.7259e-02,  2.4468e-02,  5.4468e-02, -1.3580e-01, -1.9814e-01,\n",
      "          1.2869e-01, -5.5112e-03, -1.1255e-01, -1.6407e-01,  3.0097e-02,\n",
      "          8.1781e-02,  1.1610e-01, -1.7672e-01,  1.3556e-01,  3.8671e-02,\n",
      "         -8.8466e-02,  1.7430e-01, -2.3674e-01,  6.4318e-02, -4.8065e-02,\n",
      "         -1.3547e-01, -9.0917e-02, -9.4165e-02,  2.4038e-02, -3.9397e-02,\n",
      "         -4.6271e-02, -7.9427e-02, -9.0878e-02, -1.0742e-01, -9.3367e-02,\n",
      "          4.9018e-02,  2.5076e-02,  1.7902e-01,  6.6696e-02, -2.7880e-01,\n",
      "         -1.2700e-01, -5.2775e-02,  1.1613e-01, -1.8488e-01],\n",
      "        [ 7.8299e-02,  6.6537e-02,  3.6874e-02,  1.1046e-01, -1.2381e-01,\n",
      "          1.5183e-01, -1.1428e-02,  9.6737e-02, -1.1831e-01, -1.6562e-02,\n",
      "         -4.4741e-02, -1.0695e-02, -3.1948e-02,  9.0807e-02,  5.1509e-02,\n",
      "         -1.2773e-01,  9.0835e-02, -8.6540e-02,  7.2275e-02,  1.3604e-02,\n",
      "         -9.9913e-02, -1.6167e-01,  1.8790e-01,  3.3491e-02, -3.7763e-02,\n",
      "         -6.2586e-02, -7.0510e-02, -1.9423e-02,  1.8441e-01, -2.4822e-02,\n",
      "         -7.2684e-02,  6.2827e-02, -2.1751e-01,  1.3474e-02,  9.3500e-02,\n",
      "         -1.6010e-01,  2.4536e-02, -4.6022e-02,  8.1480e-02, -5.0792e-02,\n",
      "         -1.6944e-02, -5.5083e-02, -9.3216e-02,  3.7372e-02, -3.3367e-02,\n",
      "         -8.3879e-02, -1.8057e-01,  7.6071e-02,  1.2768e-01,  6.3828e-02,\n",
      "         -1.6283e-01,  1.0254e-01,  1.1620e-01, -1.2563e-01,  7.0843e-02,\n",
      "         -1.8670e-01, -1.8316e-01, -3.7468e-02,  3.8996e-02, -1.1443e-01,\n",
      "          7.4868e-02, -1.8641e-01,  1.3862e-01,  3.3944e-02, -5.9750e-02,\n",
      "         -1.2893e-02, -2.0643e-01, -7.2866e-02, -3.8797e-02,  1.5423e-01,\n",
      "         -1.4893e-01,  5.4455e-02, -1.9904e-01, -5.3641e-02, -1.9136e-02,\n",
      "          1.7439e-01, -3.3953e-02, -1.9357e-01,  3.6549e-02,  1.0601e-01,\n",
      "          5.4116e-02, -5.7704e-02, -4.1600e-03,  9.1138e-02],\n",
      "        [ 5.2635e-02, -8.7878e-02, -1.1034e-01,  9.7204e-02, -1.5820e-01,\n",
      "          9.5366e-03, -5.2137e-03, -2.6492e-02, -6.4611e-02,  1.1916e-01,\n",
      "          1.9732e-02,  2.9404e-02, -2.5929e-02,  8.9140e-02,  1.0179e-01,\n",
      "         -3.8158e-02, -1.1871e-01,  2.5137e-02,  3.6885e-02, -2.4582e-01,\n",
      "          2.4070e-02, -3.8855e-02, -7.2179e-02, -2.8370e-03, -4.7766e-03,\n",
      "          4.3118e-02,  1.1222e-01, -1.2690e-01, -1.6231e-01, -3.2905e-02,\n",
      "          8.6106e-02, -6.5926e-02, -2.0547e-02, -5.3548e-02,  1.6672e-01,\n",
      "         -8.6684e-02, -1.3886e-01, -1.4596e-01, -6.7396e-03, -1.1146e-02,\n",
      "          8.9744e-02,  8.9581e-02,  1.0081e-01, -1.1041e-01, -7.2758e-02,\n",
      "          1.1260e-01,  9.3133e-03, -8.9185e-02, -2.1403e-02,  1.3334e-01,\n",
      "          9.2347e-03, -6.1873e-02, -1.6355e-02, -1.1622e-01,  1.0334e-02,\n",
      "         -1.1055e-01,  1.3747e-01,  1.5456e-01,  6.4167e-02,  7.5022e-02,\n",
      "          1.0991e-01,  3.4249e-02, -1.6117e-02, -1.0582e-01,  3.2901e-02,\n",
      "         -1.3661e-01,  7.2844e-02, -7.3660e-02, -1.8236e-01, -1.4045e-01,\n",
      "         -8.0243e-02,  7.8055e-02,  1.2848e-01,  1.6924e-01, -5.9846e-02,\n",
      "         -2.2837e-01,  3.2541e-02, -2.9102e-02, -6.1363e-02, -4.7741e-02,\n",
      "         -2.2057e-02, -6.9217e-02, -5.6398e-02,  2.8447e-02],\n",
      "        [ 1.1684e-01,  1.3250e-01,  5.7498e-02,  9.1477e-02,  9.9881e-02,\n",
      "         -1.1379e-02, -1.7307e-01,  4.6796e-02, -3.8532e-02, -9.7362e-02,\n",
      "         -2.2853e-02,  8.4692e-02, -4.9062e-02,  1.2224e-01, -5.9848e-02,\n",
      "         -1.2059e-01,  7.5953e-03, -4.7883e-02,  1.8292e-02, -9.5665e-03,\n",
      "         -3.9345e-02, -1.2063e-01,  1.5445e-02, -7.2560e-03, -1.3100e-01,\n",
      "         -9.6861e-02,  1.0739e-02,  4.9651e-02,  4.8558e-02,  8.6838e-02,\n",
      "         -1.5551e-01, -1.4197e-01, -5.2351e-02,  2.3484e-02, -1.2004e-02,\n",
      "          1.5438e-01, -2.0651e-01,  8.7367e-02,  1.4104e-02,  7.7772e-02,\n",
      "         -3.5568e-02,  1.1028e-01,  1.0954e-01, -5.4996e-02, -4.4541e-02,\n",
      "         -1.1208e-01, -3.2669e-02,  2.8225e-03, -1.5552e-01,  7.5855e-02,\n",
      "          3.9832e-02, -1.4219e-02,  8.3515e-02,  7.7045e-02, -2.3482e-02,\n",
      "         -1.0780e-02,  5.7781e-02, -6.2679e-02,  6.4501e-02,  1.9597e-02,\n",
      "         -8.5063e-02, -4.0785e-02,  9.8944e-02, -1.5223e-01,  8.9048e-02,\n",
      "         -5.4163e-02,  1.2247e-01,  1.3044e-01, -7.9472e-02,  6.2510e-02,\n",
      "          1.2147e-01, -5.9491e-02,  7.7379e-03,  3.8994e-02, -1.4309e-01,\n",
      "         -1.8308e-01, -2.0294e-01,  7.3602e-02,  2.9104e-02,  1.1385e-03,\n",
      "         -7.3751e-02, -5.5320e-02, -1.7625e-02, -7.0237e-02]])), ('fc3.bias', tensor([-0.1143,  0.0770, -0.1055, -0.0347,  0.0933, -0.0433, -0.0582,  0.0709,\n",
      "        -0.1005,  0.0306]))])\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing before pruning: \")\n",
    "\n",
    "# print(LeNet)\n",
    "test(model,test_loader)\n",
    "vgg = models.vgg16()\n",
    "# summary(vgg, (3, 224, 224))\n",
    "\n",
    "# print(model.state_dict())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pruning_method' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m# run pruning once\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m module,name \u001b[39min\u001b[39;00m parameters_to_prune2:\n\u001b[1;32m     20\u001b[0m   \u001b[39m# torch.nn.utils.prune.random_unstructured(module, name, num_prunedfloat)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m   torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mprune\u001b[39m.\u001b[39mglobal_unstructured(module, pruning_method, importance_scores\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_timespruned):\n\u001b[1;32m     27\u001b[0m   state \u001b[39m=\u001b[39m modelcopy2\u001b[39m.\u001b[39mstate_dict()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pruning_method' is not defined"
     ]
    }
   ],
   "source": [
    "modelcopy2= copy.deepcopy(model)\n",
    "\n",
    "parameters_to_prune2 = (\n",
    "    (modelcopy2.fc1, 'weight'),\n",
    "    (modelcopy2.fc2, 'weight'),\n",
    ")\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune2,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.2,\n",
    ")\n",
    "\n",
    "\n",
    "num_timespruned=30\n",
    "\n",
    "\n",
    "# run pruning once\n",
    "\n",
    "for module,name in parameters_to_prune2:\n",
    "  # torch.nn.utils.prune.random_unstructured(module, name, num_prunedfloat)\n",
    "  torch.nn.utils.prune.global_unstructured(module, pruning_method, importance_scores=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(num_timespruned):\n",
    "  state = modelcopy2.state_dict()\n",
    "  torch.save(state, \"modelcopy2_state.pth\")\n",
    "\n",
    "  modelretrain=  LeNet().to(device=device)\n",
    "  modelretrain.load_state_dict(torch.load(\"modelcopy2_state.pth\"))\n",
    "\n",
    "\n",
    "# fc1mask=modelcopy2.fc1\n",
    "# fc2mask=modelcopy2.fc2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing after pruning: \n",
      "Test Accuracy of the model on the 10000 test images: 1.00\n",
      "[('weight_mask', tensor([[1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 0.]]))]\n"
     ]
    }
   ],
   "source": [
    "modelcopy= copy.deepcopy(model)\n",
    "\n",
    "# max_growth = 30\n",
    "num_prunedfloat=0.2\n",
    "\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (modelcopy.fc1, 'weight'),\n",
    "    (modelcopy.fc2, 'weight'),\n",
    ")\n",
    "\n",
    "for module,name in parameters_to_prune:\n",
    "  torch.nn.utils.prune.random_unstructured(module, name, num_prunedfloat)\n",
    "\n",
    "\n",
    "print(\"Testing after pruning: \")\n",
    "test(modelcopy,test_loader)\n",
    "\n",
    "module2 = modelcopy.fc2\n",
    "# print(list(module2.named_buffers()))\n",
    "# print(list(module2.weight))\n",
    "# vgg = models.vgg16()\n",
    "# summary(vgg, (3, 224, 224))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [400/600], Loss: 0.0537\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0773\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0305\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0182\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 34\u001b[0m\n\u001b[1;32m     22\u001b[0m baseline_acc \u001b[39m=\u001b[39m []\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[1;32m     24\u001b[0m   \u001b[39m#grow 1/i*growth_factor params total:\u001b[39;00m\n\u001b[1;32m     25\u001b[0m   \n\u001b[1;32m     26\u001b[0m   \u001b[39m#for module,name in parameters_to_prune:\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[39m#grow\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m   train(model, train_loader,test_loader,num_epochs \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m, optimizer \u001b[39m=\u001b[39;49m optimizer)\n\u001b[1;32m     35\u001b[0m   \u001b[39m# train(baseline, train_loader,test_loader,num_epochs = 5, optimizer = baseline_optim)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m   \u001b[39m# model_acc.append(test(model, test_loader))\u001b[39;00m\n\u001b[1;32m     37\u001b[0m   baseline_acc\u001b[39m.\u001b[39mappend(test(baseline, test_loader))\n",
      "Cell \u001b[0;32mIn[3], line 53\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, num_epochs, optimizer)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m# Backward and optimize\u001b[39;00m\n\u001b[1;32m     52\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 53\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     54\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     56\u001b[0m \u001b[39mif\u001b[39;00m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m400\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/me-Programming/envirs/tensorflow-test/env/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/me-Programming/envirs/tensorflow-test/env/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_growth = 20\n",
    "num_epochs = 10\n",
    "gamma = 10 #num new nodes on first growth\n",
    "\n",
    "model = LeNet(max_growth = max_growth)\n",
    "baseline = LeNet(max_growth = 0)\n",
    "\n",
    "parameters_to_prune = (\n",
    "#    (model.conv1, 'weight'),\n",
    "#    (model.conv2, 'weight'),\n",
    "    (model.fc1, 'weight'),\n",
    "    (model.fc2, 'weight'),\n",
    "#    (model.fc3, 'weight'),\n",
    ")\n",
    "for module,name in parameters_to_prune:\n",
    "  torch.nn.utils.prune.random_structured(module, name, max_growth,  0)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "baseline_optim = torch.optim.Adam(baseline.parameters(), lr=learning_rate)\n",
    "\n",
    "model_acc = []\n",
    "baseline_acc = []\n",
    "for i in range(5):\n",
    "  #grow 1/i*growth_factor params total:\n",
    "  \n",
    "  #for module,name in parameters_to_prune:\n",
    "    #grow\n",
    "  \n",
    "    \n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "  train(model, train_loader,test_loader,num_epochs = 5, optimizer = optimizer)\n",
    "  # train(baseline, train_loader,test_loader,num_epochs = 5, optimizer = baseline_optim)\n",
    "  # model_acc.append(test(model, test_loader))\n",
    "  baseline_acc.append(test(baseline, test_loader))\n",
    "\n",
    "  plt.plot(np.arange(len(model_acc)), model_acc, label = 'Adaptive Model')\n",
    "  plt.plot(np.arange(len(baseline_acc)), baseline_acc, label = 'Baseline')\n",
    "  plt.show()\n",
    "\n",
    "  prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.10+(-1*i*.01),\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters_to_prune2 = (\n",
    "#     (model.fc1, 'weight'),\n",
    "#     (model.fc2, 'weight'),\n",
    "# )\n",
    "# prune.global_unstructured(\n",
    "#     parameters_to_prune2,\n",
    "#     pruning_method=prune.L1Unstructured,\n",
    "#     amount=0.5\n",
    "# )\n",
    "# # run pruning once\n",
    "# # prune.global_unstructured.pruning_method\n",
    "#   # torch.nn.utils.prune.random_unstructured(module, name, num_prunedfloat)\n",
    "# prune.global_unstructured(parameters_to_prune2,prune.L1Unstructured, importance_scores=None, amount=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "065903266eccafe089f2ae6c8a75912f88be57d9e6a3d93d1ff3f958981ce42e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
