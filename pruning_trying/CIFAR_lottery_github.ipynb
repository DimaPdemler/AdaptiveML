{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6d7c03fb10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "\n",
    "# from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, max_growth):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc1 = nn.Linear(400,500+max_growth)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(500+max_growth,100+max_growth)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(100+max_growth,10)\n",
    "\n",
    "        #Custom Functionality\n",
    "        self.max_growth = max_growth\n",
    "        previous_module = self.parameters_to_prune()[0][0]\n",
    "\n",
    "        for module, name in self.parameters_to_prune()[1:]:\n",
    "\n",
    "          #Prune incoming weights (Previous layers outgoing weights)\n",
    "          if previous_module != None:\n",
    "            mask = torch.ones(np.shape(previous_module.weight))\n",
    "            mask[-max_growth:,:] = 0\n",
    "            prune.custom_from_mask(previous_module, name, mask)\n",
    "            \n",
    "          #Prune outgoing weights\n",
    "          mask = torch.ones(np.shape(module.weight))\n",
    "          mask[:,-max_growth:] = 0\n",
    "          prune.custom_from_mask(module, name, mask)\n",
    "\n",
    "          previous_module = module  #save layer for next iteration\n",
    "        \n",
    "\n",
    "    def parameters_to_prune(self):\n",
    "        return ((self.fc1, 'weight'),\n",
    "                (self.fc2, 'weight'),\n",
    "                (self.fc3, 'weight'),) \n",
    "    #End Custom Functionality\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define relevant variables for the ML task\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "#Loading the dataset and preprocessing\n",
    "train_dataset = torchvision.datasets.CIFAR10(root = '../cifar/',\n",
    "                                           train = True,\n",
    "                                           transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
    "                                           download = True)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root = '../cifar/',\n",
    "                                          train = False,\n",
    "                                          transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
    "                                          download=True)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "augmented_train_dataset = torchvision.datasets.CIFAR10(root = '../cifar/',\n",
    "                                           train = True,\n",
    "                                           transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.RandomVerticalFlip(p=1),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
    "                                           download = True)\n",
    "\n",
    "augmented_train_loader = torch.utils.data.DataLoader(dataset = augmented_train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "augmented_test_dataset = torchvision.datasets.CIFAR10(root = '../cifar/',\n",
    "                                          train = False,\n",
    "                                          transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.RandomVerticalFlip(p=1),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
    "                                          download=True)\n",
    "\n",
    "\n",
    "augmented_test_loader = torch.utils.data.DataLoader(dataset = augmented_test_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "\n",
    "# train_dataset, test_dataset = train_dataset.cuda(), test_dataset.cuda()\n",
    "\n",
    "#Setting the loss function\n",
    "cost = nn.CrossEntropyLoss()\n",
    "\n",
    "#this is defined to print how many steps are remaining when training\n",
    "total_step = len(train_loader)\n",
    "\n",
    "def train(model,train_loader,test_loader,num_epochs,optimizer):\n",
    "  accuracy=[]\n",
    "#   model.to(device)\n",
    "  total_step = len(train_loader)\n",
    "  for epoch in range(num_epochs):\n",
    "    \n",
    "      for i, (images, labels) in enumerate(train_loader): \n",
    "          \n",
    "          images = images.to(device)\n",
    "          labels = labels.to(device)\n",
    "          \n",
    "          #Forward pass\n",
    "          outputs = model(images)\n",
    "          loss = cost(outputs, labels)\n",
    "            \n",
    "          # Backward and optimize\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "              \n",
    "          if (i+1) % 400 == 0:\n",
    "              print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                        .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "# def test(model, test_loader):\n",
    "#     # Test the model\n",
    "#     model.to(device)\n",
    "#     test_loader.to(device)\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "        \n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         for i, (images, labels) in enumerate(test_loader):  \n",
    "            \n",
    "#             test_output = model(images)\n",
    "#             pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "#             correct += (pred_y == labels).sum().item()\n",
    "#             total += labels.size(0)\n",
    "#         accuracy = correct / total\n",
    "\n",
    "#     print('Test Accuracy of the model on the 10000 test images: ', accuracy)\n",
    "#     return accuracy\n",
    "\n",
    "def test(model, test_loader):\n",
    "    print(\"model: \",type(model))\n",
    "    print(\"test_loader: \",type(test_loader))\n",
    "    # model.to(device)\n",
    "    # test_loader.to(device)\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(test_loader): \n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            # model=model.to(device)\n",
    "            test_output = model(images)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: %.2f' % accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All in One Method to Find Lottery Tickets given a model and dataset, saves models in shared drive\n",
    "def find_ticket(model, name, train_loader, test_loader, start_iter = 0, end_iter = 30, num_epochs = 10, learning_rate = .001, prune_amount = .2, k = 3):\n",
    "  model.to(device)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  #If training has already been saved\n",
    "  try:\n",
    "    model.load_state_dict(torch.load('../cifar/models/'+ name + '_RewindWeights' + '_' + str(k)))\n",
    "  except:\n",
    "    rand=train(model, train_loader,test_loader,num_epochs = k, optimizer = optimizer)  #Save Kth epoch model\n",
    "    torch.save(model.state_dict(), '../cifar/models/' + name + '_RewindWeights' + '_' + str(k))\n",
    "\n",
    "  model_rewind = LeNet(max_growth = model.max_growth) #Save as separate model so we can rewind our weights back to this\n",
    "  model_rewind.load_state_dict(torch.load('../cifar/models/'+ name + '_RewindWeights' + '_' + str(k)))\n",
    "\n",
    "  rand=train(model, train_loader,test_loader,num_epochs = num_epochs - k, optimizer = optimizer) #Finish off training\n",
    "  accuracy = []\n",
    "\n",
    "  for i in range(start_iter, end_iter): \n",
    "    #Prune\n",
    "    prune.global_unstructured(model.parameters_to_prune(),pruning_method=prune.L1Unstructured,amount=prune_amount,)\n",
    "    \n",
    "    #Rewind Weights\n",
    "    for idx, (module, _) in enumerate(model.parameters_to_prune()):\n",
    "      with torch.no_grad():\n",
    "        module_rewind = model_rewind.parameters_to_prune()[idx][0]\n",
    "        module.weight_orig.copy_(module_rewind.weight)\n",
    "\n",
    "    print(\n",
    "    \"Global Sparsity: {:.2f}%\".format(\n",
    "        100. * (float(torch.sum(model.fc3.weight == 0)) + float(torch.sum(model.fc1.weight == 0)) + float(torch.sum(model.fc2.weight == 0)))\n",
    "        / (float(model.fc3.weight.nelement()) + float(model.fc1.weight.nelement()) + float(model.fc2.weight.nelement()))\n",
    "    )\n",
    "    )\n",
    "    \n",
    "    #Train\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train(model, train_loader,test_loader,num_epochs = num_epochs, optimizer = optimizer)\n",
    "    accuracy.append(test(model, test_loader))\n",
    "\n",
    "    plt.plot(np.arange(len(accuracy)), accuracy)\n",
    "    plt.show()\n",
    "    print('Saving iteration ', str(i+1))\n",
    "    torch.save(model.state_dict(), '../cifar/models/' + name + '_iter' + str(i+1)) \n",
    "\n",
    "def grow(model, parameters_to_prune, amount):\n",
    "  previous_module = parameters_to_prune[0][0]\n",
    "\n",
    "  for module, name in parameters_to_prune[1:]:\n",
    "    mask = module.get_buffer('weight_mask').data\n",
    "    #Pick Suitable Locations\n",
    "    omega = []\n",
    "    for idx, val in enumerate(torch.sum(mask, dim = 0)): \n",
    "      if val == 0:\n",
    "        omega.append(idx)\n",
    "\n",
    "    indices = np.random.choice(omega, size = min(len(omega), amount), replace = False)\n",
    "    \n",
    "    #Grow at these indices\n",
    "    module.get_buffer('weight_mask')[:,indices] = 1\n",
    "    prune.custom_from_mask(module,'weight', torch.ones(module.weight.size()))\n",
    "\n",
    "    previous_module.get_buffer('weight_mask')[indices,:] = 1\n",
    "    prune.custom_from_mask(previous_module,'weight', torch.ones(previous_module.weight.size()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [400/782], Loss: 1.3261\n",
      "Epoch [2/50], Step [400/782], Loss: 1.2057\n",
      "Epoch [3/50], Step [400/782], Loss: 1.2589\n",
      "Epoch [4/50], Step [400/782], Loss: 1.3355\n",
      "Epoch [5/50], Step [400/782], Loss: 0.9150\n",
      "Epoch [6/50], Step [400/782], Loss: 0.9570\n",
      "Epoch [7/50], Step [400/782], Loss: 0.7403\n",
      "Epoch [8/50], Step [400/782], Loss: 0.5793\n",
      "Epoch [9/50], Step [400/782], Loss: 0.5853\n",
      "Epoch [10/50], Step [400/782], Loss: 0.5546\n",
      "Epoch [11/50], Step [400/782], Loss: 0.4632\n",
      "Epoch [12/50], Step [400/782], Loss: 0.6042\n",
      "Epoch [13/50], Step [400/782], Loss: 0.4601\n",
      "Epoch [14/50], Step [400/782], Loss: 0.2288\n",
      "Epoch [15/50], Step [400/782], Loss: 0.1908\n",
      "Epoch [16/50], Step [400/782], Loss: 0.3133\n",
      "Epoch [17/50], Step [400/782], Loss: 0.3765\n",
      "Epoch [18/50], Step [400/782], Loss: 0.2014\n",
      "Epoch [19/50], Step [400/782], Loss: 0.1064\n",
      "Epoch [20/50], Step [400/782], Loss: 0.1882\n",
      "Epoch [21/50], Step [400/782], Loss: 0.1383\n",
      "Epoch [22/50], Step [400/782], Loss: 0.3325\n",
      "Epoch [23/50], Step [400/782], Loss: 0.3058\n",
      "Epoch [24/50], Step [400/782], Loss: 0.0514\n",
      "Epoch [25/50], Step [400/782], Loss: 0.1125\n",
      "Epoch [26/50], Step [400/782], Loss: 0.0655\n",
      "Epoch [27/50], Step [400/782], Loss: 0.0824\n",
      "Epoch [28/50], Step [400/782], Loss: 0.0395\n",
      "Epoch [29/50], Step [400/782], Loss: 0.0348\n",
      "Epoch [30/50], Step [400/782], Loss: 0.1619\n",
      "Epoch [31/50], Step [400/782], Loss: 0.1542\n",
      "Epoch [32/50], Step [400/782], Loss: 0.0359\n",
      "Epoch [33/50], Step [400/782], Loss: 0.0442\n",
      "Epoch [34/50], Step [400/782], Loss: 0.0988\n",
      "Epoch [35/50], Step [400/782], Loss: 0.1639\n",
      "Epoch [36/50], Step [400/782], Loss: 0.0913\n",
      "Epoch [37/50], Step [400/782], Loss: 0.1168\n",
      "Epoch [38/50], Step [400/782], Loss: 0.1005\n",
      "Epoch [39/50], Step [400/782], Loss: 0.0464\n",
      "Epoch [40/50], Step [400/782], Loss: 0.0676\n",
      "Epoch [41/50], Step [400/782], Loss: 0.0490\n",
      "Epoch [42/50], Step [400/782], Loss: 0.0404\n",
      "Epoch [43/50], Step [400/782], Loss: 0.1613\n",
      "Epoch [44/50], Step [400/782], Loss: 0.0266\n",
      "Epoch [45/50], Step [400/782], Loss: 0.0686\n",
      "Epoch [46/50], Step [400/782], Loss: 0.0096\n",
      "Epoch [47/50], Step [400/782], Loss: 0.0824\n",
      "Epoch [48/50], Step [400/782], Loss: 0.0351\n",
      "Epoch [49/50], Step [400/782], Loss: 0.0087\n",
      "Epoch [50/50], Step [400/782], Loss: 0.0896\n"
     ]
    }
   ],
   "source": [
    "max_growth = 20\n",
    "num_epochs = 50\n",
    "gamma = 10 #num new nodes on first growth\n",
    "\n",
    "model2 = LeNet(max_growth = max_growth)\n",
    "model2=model2.to(device)\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model2, train_loader,test_loader,num_epochs, optimizer = optimizer)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  <class '__main__.LeNet'>\n",
      "test_loader:  <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Test Accuracy of the model on the 10000 test images: 0.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model2,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/9], Step [400/782], Loss: 0.8321\n",
      "Epoch [2/9], Step [400/782], Loss: 1.0670\n",
      "Epoch [3/9], Step [400/782], Loss: 0.7431\n",
      "Epoch [4/9], Step [400/782], Loss: 0.7575\n",
      "Epoch [5/9], Step [400/782], Loss: 0.5265\n",
      "Epoch [6/9], Step [400/782], Loss: 0.7223\n",
      "Epoch [7/9], Step [400/782], Loss: 0.5056\n",
      "Epoch [8/9], Step [400/782], Loss: 0.5852\n",
      "Epoch [9/9], Step [400/782], Loss: 0.4226\n",
      "Global Sparsity: 33.95%\n",
      "Epoch [1/12], Step [400/782], Loss: 1.0812\n",
      "Epoch [2/12], Step [400/782], Loss: 0.7996\n",
      "Epoch [3/12], Step [400/782], Loss: 0.7455\n",
      "Epoch [4/12], Step [400/782], Loss: 0.7718\n",
      "Epoch [5/12], Step [400/782], Loss: 0.8408\n",
      "Epoch [6/12], Step [400/782], Loss: 0.8114\n",
      "Epoch [7/12], Step [400/782], Loss: 0.7588\n"
     ]
    }
   ],
   "source": [
    "#Generate Tickets\n",
    "\n",
    "max_growth = 50\n",
    "model = LeNet(max_growth = max_growth)\n",
    "find_ticket(model, 'CIFAR10', train_loader, test_loader, num_epochs=12, prune_amount=0.2, end_iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "065903266eccafe089f2ae6c8a75912f88be57d9e6a3d93d1ff3f958981ce42e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
